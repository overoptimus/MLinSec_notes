{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.3317e+17,  4.5869e-41,  9.2198e-39],\n",
      "        [ 1.8381e-40,  0.0000e+00,  2.5244e-29],\n",
      "        [ 0.0000e+00,  2.5244e-29,  1.8361e+25],\n",
      "        [ 1.4603e-19,  6.4069e+02,  2.7489e+20],\n",
      "        [ 1.5444e+25,  1.6217e-19,  7.0062e+22]])\n"
     ]
    }
   ],
   "source": [
    "#构造一个未初始化的矩阵\n",
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0777, 0.8518, 0.1810],\n",
      "        [0.4824, 0.6920, 0.5830],\n",
      "        [0.0606, 0.6420, 0.3073],\n",
      "        [0.0935, 0.7858, 0.7351],\n",
      "        [0.0919, 0.4016, 0.8670]])\n"
     ]
    }
   ],
   "source": [
    "#构造一个随机初始化的矩阵\n",
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造一个0填充的且dtype long的矩阵\n",
    "x = torch.zeros(5,3,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.4000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "#直接从数据构造张量\n",
    "x = torch.tensor([4.4,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3,dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9875,  0.0732,  1.0035],\n",
       "        [-0.7549, -0.8295, -0.0205],\n",
       "        [-0.1700, -0.8002,  0.0171],\n",
       "        [-1.8364,  0.2177,  0.2655],\n",
       "        [-1.7763,  1.6139, -0.9225]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量的大小\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9875,  0.0732,  1.0035],\n",
      "        [-0.7549, -0.8295, -0.0205],\n",
      "        [-0.1700, -0.8002,  0.0171],\n",
      "        [-1.8364,  0.2177,  0.2655],\n",
      "        [-1.7763,  1.6139, -0.9225]])\n",
      "tensor([[0.1957, 0.8940, 0.1708],\n",
      "        [0.1986, 0.9980, 0.2928],\n",
      "        [0.3862, 0.8003, 0.1611],\n",
      "        [0.7501, 0.8581, 0.7240],\n",
      "        [0.0702, 0.3595, 0.4619]])\n",
      "tensor([[ 2.1832e+00,  9.6720e-01,  1.1743e+00],\n",
      "        [-5.5635e-01,  1.6852e-01,  2.7237e-01],\n",
      "        [ 2.1619e-01,  5.2154e-05,  1.7820e-01],\n",
      "        [-1.0862e+00,  1.0758e+00,  9.8952e-01],\n",
      "        [-1.7061e+00,  1.9734e+00, -4.6067e-01]])\n",
      "加法2\n",
      "tensor([[ 2.1832e+00,  9.6720e-01,  1.1743e+00],\n",
      "        [-5.5635e-01,  1.6852e-01,  2.7237e-01],\n",
      "        [ 2.1619e-01,  5.2154e-05,  1.7820e-01],\n",
      "        [-1.0862e+00,  1.0758e+00,  9.8952e-01],\n",
      "        [-1.7061e+00,  1.9734e+00, -4.6067e-01]])\n"
     ]
    }
   ],
   "source": [
    "#加法\n",
    "y = torch.rand(5,3)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x+y)\n",
    "#加法2\n",
    "print('加法2')\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1832e+00,  9.6720e-01,  1.1743e+00],\n",
      "        [-5.5635e-01,  1.6852e-01,  2.7237e-01],\n",
      "        [ 2.1619e-01,  5.2154e-05,  1.7820e-01],\n",
      "        [-1.0862e+00,  1.0758e+00,  9.8952e-01],\n",
      "        [-1.7061e+00,  1.9734e+00, -4.6067e-01]])\n"
     ]
    }
   ],
   "source": [
    "#提供一个参数，作为计算结果的接收\n",
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add:tensor([[0.1957, 0.8940, 0.1708],\n",
      "        [0.1986, 0.9980, 0.2928],\n",
      "        [0.3862, 0.8003, 0.1611],\n",
      "        [0.7501, 0.8581, 0.7240],\n",
      "        [0.0702, 0.3595, 0.4619]])\n",
      "add_tensor([[0.1957, 0.8940, 0.1708],\n",
      "        [0.1986, 0.9980, 0.2928],\n",
      "        [0.3862, 0.8003, 0.1611],\n",
      "        [0.7501, 0.8581, 0.7240],\n",
      "        [0.0702, 0.3595, 0.4619]])\n"
     ]
    }
   ],
   "source": [
    "#直接将一个张量加到另一个张量上\n",
    "print('add:',end='')\n",
    "y.add(x)\n",
    "print(y)\n",
    "print('add_:',end='')# 就地改变y的值\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9875,  0.0732,  1.0035],\n",
      "        [-0.7549, -0.8295, -0.0205],\n",
      "        [-0.1700, -0.8002,  0.0171],\n",
      "        [-1.8364,  0.2177,  0.2655],\n",
      "        [-1.7763,  1.6139, -0.9225]])\n",
      "tensor([ 1.0035, -0.0205,  0.0171,  0.2655, -0.9225])\n",
      "tensor([ 1.9875, -0.7549, -0.1700, -1.8364, -1.7763])\n",
      "tensor([[-1.7763,  1.6139, -0.9225]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[:,-1])\n",
    "print(x[:,0])\n",
    "print(x[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "#调整张量的大小和形状\n",
    "x = torch.rand(4,4) #均匀分布[0,1)之间\n",
    "x = torch.randn(4,4)#标准正态分布，均值为0，方差为0\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) #这里的-1意思是通过其他的尺寸来推算该位置的值，这里就是说通过总共大小16和一个方向为8，推算该位置为几。\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20025917887687683"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#通过.item()将张量的值作为python数字获取\n",
    "x = torch.randn(1)\n",
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1447744369506836"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,2)\n",
    "x[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# numpy bridge\n",
    "# 将torch张量转换为numpy数组，反之亦然\n",
    "# torch tensor和numpy数组将共享内存位置（如果tensor在cpu上）\n",
    "\n",
    "# tensor转换为numpy\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# numpy转换为tensor\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autograd:自动微分（automatic differentiation）\n",
    "'''\n",
    "该autograd包是pytorch中所有神经的核心。让我们简单地访问它，然后\n",
    "训练我们第一个基于torch的神经网络。\n",
    "该autograd软件包为tensor上的所有操作提供自动微分。他是一个由运行定义的框架，\n",
    "这意味着以代码运行方式定义你的后向传播，并且每次迭代都可以不同。我们从tensor和gradients来举一些例子。\n",
    "\n",
    "1. 张量\n",
    "torch.tensor是程序的中心类。如果将其属性设置.requires_grad为Ture，它将开始跟踪对其的所有操作。\n",
    "完成计算后，可以调用.backward()并自动计算所有的梯度。该张量的梯度将累加到.grad属性中。\n",
    "要停止张量跟踪历史记录，调用.detach()将其从计算历史记录中分离出来，并防止跟踪将来\n",
    "的计算。\n",
    "要停止跟踪历史记录（和使用内存），还可以将代码块包装在torch.no_grad()中：这在评估模型时特别有\n",
    "用，因为模型的可训练参数有requires_grad=Ture，但我们在评估阶段不需要它梯度。\n",
    "\n",
    "还有一个类对autograd实现非常重要那就是Function。Tensor和Function相互连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。\n",
    "每个张量都有一个.grad_fn属性保存着创建了张量的Function的引用，（如果用户自己创建张量，则grad_fn是None）。\n",
    "\n",
    "如果向计算倒数，可以调用Tensor.backward()。如果Tensor是标量（即它包含一个元素数据），则不需要制定任何参数backward（），但是如果他有更多元素，则需要指定\n",
    "一个gradient参数来指定张量的形状。\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}